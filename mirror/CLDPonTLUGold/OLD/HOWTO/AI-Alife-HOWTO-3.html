<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML>
<HEAD>
 <META NAME="GENERATOR" CONTENT="SGML-Tools 1.0.9">
 <TITLE>GNU/Linux AI &amp; Alife HOWTO: Connectionism</TITLE>
 <LINK HREF="AI-Alife-HOWTO-4.html" REL=next>
 <LINK HREF="AI-Alife-HOWTO-2.html" REL=previous>
 <LINK HREF="AI-Alife-HOWTO.html#toc3" REL=contents>
<SCRIPT src="../menu.js"> function BeginPage() {} function EndPage() {} </SCRIPT> </HEAD> <BODY bgcolor=#EEEEFF MARGINHEIGHT=0 MARGINWIDTH=0> <SCRIPT>BeginPage(1, 8, 1);</SCRIPT>
<A HREF="AI-Alife-HOWTO-4.html"><IMG SRC="../img/next.gif" ALT="Next"></A>
<A HREF="AI-Alife-HOWTO-2.html"><IMG SRC="../img/prev.gif" ALT="Previous"></A>
<A HREF="AI-Alife-HOWTO.html#toc3"><IMG SRC="../img/toc.gif" ALT="Contents"></A>
<HR>
<H2><A NAME="s3">3. Connectionism</A></H2>

<P>Connectionism is a technical term for a group of related
techniques. These techniques include areas such as Artificial
Neural Networks, Semantic Networks and a few other similar
ideas. My present focus is on neural networks (though I am
looking for resources on the other techniques). Neural
networks are programs designed to simulate the workings of the
brain. They consist of a network of small mathematical-based
nodes, which work together to form patterns of information.
They have tremendous potential and currently seem to be having
a great deal of success with image processing and robot
control.
<P>
<P>
<H2><A NAME="ss3.1">3.1 Connectionist class/code libraries</A>
</H2>

<P>
<P>These are libraries of code or classes for use in programming within
the Connectionist field.  They are not meant as stand alone
applications, but rather as tools for building your own applications.
<P>
<DL>
<P>
<P>
<A NAME="ANSI-C ANN"></A> <DT><B>ANSI-C Neural Networks</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.geocities.com/CapeCanaveral/1624/">www.geocities.com/CapeCanaveral/1624/</A></LI>
</UL>
<P>
<P>This site contains ANSC-C source code for 8 types of neural
nets, including:
<UL>
<LI>Adaline Network </LI>
<LI>Backpropagation</LI>
<LI>Hopfield Model</LI>
<LI>(BAM) Bidirectional Associative Memory</LI>
<LI>Boltzmann Machine</LI>
<LI>Counterpropagation</LI>
<LI>(SOM) Self-Organizing Map</LI>
<LI>(ART1) Adaptive Resonance Theory</LI>
</UL>
<P>They were designed to help turn the theory of a particular
network model into the design for a simulator implementation ,
and to help with embeding an actual application into a
particular network model.
<P>
<P>
<P>
<P>
<A NAME="BELIEF"></A> <DT><B>BELIEF</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/reasonng/probabl/belief/">www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/reasonng/probabl/belief/</A></LI>
</UL>
<P>
<P>BELIEF is a Common Lisp implementation of the Dempster and Kong
fusion and propagation algorithm for Graphical Belief Function
Models and the Lauritzen and Spiegelhalter algorithm for
Graphical Probabilistic Models. It includes code for
manipulating graphical belief models such as Bayes Nets and
Relevance Diagrams (a subset of Influence Diagrams) using both
belief functions and probabilities as basic representations of
uncertainty. It uses the Shenoy and Shafer version of the
algorithm, so one of its unique features is that it supports
both probability distributions and belief functions.  It also
has limited support for second order models (probability
distributions on parameters).
<P>
<A NAME="bpnn.py"></A> <DT><B>bpnn.py</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.enme.ucalgary.ca/~nascheme/python/">www.enme.ucalgary.ca/~nascheme/python/</A></LI>
</UL>
<P>A simple back-propogation ANN in Python.
<P>
<P>
<A NAME="CONICAL"></A> <DT><B>CONICAL</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://strout.net/conical/">strout.net/conical/</A></LI>
</UL>
<P>CONICAL is a C++ class library for building simulations common
in computational neuroscience. Currently its focus is on
compartmental modeling, with capabilities similar to GENESIS and
NEURON. A model neuron is built out of compartments, usually
with a cylindrical shape. When small enough, these open-ended
cylinders can approximate nearly any geometry. Future classes
may support reaction-diffusion kinetics and more. A key feature
of CONICAL is its cross-platform compatibility; it has been
fully co-developed and tested under Unix, DOS, and Mac OS.
<P>
<P>
<P>
<A NAME="IDEAL"></A> <DT><B>IDEAL</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.rpal.rockwell.com/ideal.html">www.rpal.rockwell.com/ideal.html</A></LI>
</UL>
<P>IDEAL is a test bed for work in influence diagrams and
Bayesian networks. It contains various inference algorithms
for belief networks and evaluation algorithms for influence
diagrams. It contains facilities for creating and editing
influence diagrams and belief networks.
<P>IDEAL is written in pure Common Lisp and so it will run in
Common Lisp on any platform. The emphasis in writing IDEAL has
been on code clarity and providing high level programming
abstractions. It thus is very suitable for experimental
implementations which need or extend belief network
technology.
<P>At the highest level, IDEAL can be used as a subroutine
library which provides belief network inference and influence
diagram evaluation as a package. The code is documented in a
detailed manual and so it is also possible to work at a lower
level on extensions of belief network methods.
<P>IDEAL comes with an optional graphic interface written in
CLIM. If your Common Lisp also has CLIM, you can run the
graphic interface.
<P>
<P>
<A NAME="Jet's Neural Architecture"></A> <DT><B>Jet's Neural Architecture</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.voltar-confed.org/jneural/">www.voltar-confed.org/jneural/</A></LI>
</UL>
<P>Jet's Neural Architecture is a C++ framework for doing neural net
projects. The goals of this project were to make a fast, flexible
neural architecture that isn't stuck to one kind of net and to make
sure that end users could easily write useful applications. All the
documentation is also easily readable.
<P>
<P>
<P>
<A NAME="Matrix Class"></A> <DT><B>Matrix Class</B><DD><P>
<UL>
<LI>FTP site: 
<A HREF="ftp://ftp.cs.ucla.edu/pub/">ftp.cs.ucla.edu/pub/</A></LI>
</UL>
<P>A simple, fast, efficient C++ Matrix class designed for
scientists and engineers. The Matrix class is well suited for
applications with complex math algorithms. As an demonstration
of the Matrix class, it was used to implement the backward error
propagation algorithm for a multi-layer feed-forward artificial
neural network.
<P>
<P>
<A NAME="nunu"></A> <DT><B>nunu</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://ruby.ddiworld.com/jreed/web/software/nn.html">ruby.ddiworld.com/jreed/web/software/nn.html</A></LI>
</UL>
<P>nunu is a multi-layered, scriptable, back-propagation neural network.
It is build to be used for intensive computation problems scripted
in shell scripts. It is written in C++ using the STL. nn is based
on material from the "Introduction to the Theory of Neural
Computation" by John Hertz, Anders Krogh, and Richard G. Palmer,
chapter 6.
<P>
<P>
<A NAME="Pulcinella"></A> <DT><B>Pulcinella</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://iridia.ulb.ac.be/pulcinella/Welcome.html">iridia.ulb.ac.be/pulcinella/Welcome.html</A></LI>
</UL>
<P>
<P>Pulcinella is written in CommonLisp, and appears as a library of
Lisp functions for creating, modifying and evaluating valuation
systems. Alternatively, the user can choose to interact with
Pulcinella via a graphical interface (only available in Allegro
CL). Pulcinella provides primitives to build and evaluate
uncertainty models according to several uncertainty calculi,
including probability theory, possibility theory, and
Dempster-Shafer's theory of belief functions; and the
possibility theory by Zadeh, Dubois and Prade's. A User's Manual
is available on request.
<P>
<P>
<P>
<A NAME="S-ElimBel"></A> <DT><B>S-ElimBel</B><DD><P>
<UL>
<LI>Web site (???): 
<A HREF="http://www.spaces.uci.edu/thiery/elimbel/">www.spaces.uci.edu/thiery/elimbel/</A></LI>
</UL>
<P>
<P>S-ElimBel is an algorithm that computes the belief in a
Bayesian network, implemented in MIT-Scheme. This algorithm has
the particularity of being rather easy to understand. Moreover,
one can apply it to any kind of Bayesian network - it being
singly connected or muliply connected. It is, however, less
powerful than the standard algorithm of belief propagation.
Indeed, the computation has to be reconducted entirely for each
new evidence added to the network. Also, one needs to run the
algorithm as many times as one has nodes for which the belief is
wanted.
<P>
<P>
<P>
<A NAME="Baysian Modeling"></A> <DT><B>Software for Flexible Bayesian Modeling</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.cs.utoronto.ca/~radford/fbm.software.html">www.cs.utoronto.ca/~radford/fbm.software.html</A></LI>
</UL>
<P>
<P>This software implements flexible Bayesian models for regression
and classification applications that are based on multilayer
perceptron neural networks or on Gaussian processes.  The
implementation uses Markov chain Monte Carlo methods.  Software
modules that support Markov chain sampling are included in the
distribution, and may be useful in other applications.
<P>
<P>
<P>
<P>
<A NAME="Spiderweb2"></A> <DT><B>Spiderweb2</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.cs.nyu.edu/~klap7794/spiderweb2.html">www.cs.nyu.edu/~klap7794/spiderweb2.html</A></LI>
</UL>
<P>
<P>A C++ artificial neual net library.  Spiderweb2 is a complete
rewrite of the original Spiderweb library, it has grown into a
much more flexible and object-oriented system. The biggest
change is that each neuron object is responsible for its own
activations and updates, with the network providing only the
scheduling aspect. This is a very powerful change, and it allows
easy modification and experimentation with various network
architectures and neuron types.
<P>
<P>
<P>
<A NAME="SPI"></A> <DT><B>Symbolic Probabilistic Inference (SPI)</B><DD><P>
<UL>
<LI>FTP site: 
<A HREF="ftp://ftp.engr.orst.edu/pub/dambrosi/spi/">ftp.engr.orst.edu/pub/dambrosi/spi/</A></LI>
<LI>Paper (ijar-94.ps): 
<A HREF="ftp://ftp.engr.orst.edu/pub/dambrosi/">ftp.engr.orst.edu/pub/dambrosi/</A></LI>
</UL>
<P>
<P>Contains Common Lisp function libraries to implement SPI type baysean nets. 
Documentation is very limited.
Features: 
<UL>
<LI>Probabilities, Local Expression Language Utilities, Explanation, 
Dynamic Models, and a TCL/TK based GUI.</LI>
</UL>
<P>
<P>
<P>
<A NAME="TresBel"></A> <DT><B>TresBel</B><DD><P>
<UL>
<LI>FTP site: 
<A HREF="ftp://iridia.ulb.ac.be/pub/hongxu/software/">iridia.ulb.ac.be/pub/hongxu/software/</A></LI>
</UL>
<P>
<P>Libraries containing (Allegro) Common Lisp code for Belief Functions 
(aka. Dempster-Shafer evidential reasoning) as a representation 
of uncertainty. Very little documentation. Has a limited GUI.
<P>
<P>
<P>
<A NAME="C++ ANNs"></A> <DT><B>Various (C++) Neural Networks</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.dontveter.com/nnsoft/nnsoft.html">www.dontveter.com/nnsoft/nnsoft.html</A></LI>
</UL>
<P>
<P>Example neural net codes from the book, 
<A HREF="http://www.dontveter.com/basisofai/basisofai.html">The       Pattern Recognition Basics of AI</A>.
These are simple example codes of these various
neural nets. They work well as a good starting point for simple
experimentation and for learning what the code is like behind the
simulators. The types of networks available on this site are: 
(implemented in C++)
<P>
<P>
<UL>
<LI>The Backprop Package</LI>
<LI>The Nearest Neighbor Algorithms</LI>
<LI>The Interactive Activation Algorithm</LI>
<LI>The Hopfield and Boltzman machine Algorithms</LI>
<LI>The Linear Pattern Classifier</LI>
<LI>ART I</LI>
<LI>Bi-Directional Associative Memory</LI>
<LI>The Feedforward Counter-Propagation Network</LI>
</UL>
<P>
<P>
<P>
</DL>
<P>
<H2><A NAME="ss3.2">3.2 Connectionist software kits/applications</A>
    </H2>

<P>
<P>These are various applications, software kits, etc. meant for research
in the field of Connectionism. Their ease of use will vary, as they
were designed to meet some particular research interest more than as
an easy to use commercial package.
<DL>
<P>
<P>
<A NAME="Aspirin-MIGRANES"></A> <DT><B>Aspirin - MIGRAINES</B><DD><P>(am6.tar.Z on ftp site)
<UL>
<LI>FTP site: 
<A HREF="ftp://sunsite.unc.edu/pub/academic/computer-science/neural-networks/programs/Aspirin/">sunsite.unc.edu/pub/academic/computer-science/neural-networks/programs/Aspirin/</A></LI>
</UL>
<P>
<P>The software that we are releasing now is for creating, 
and evaluating, feed-forward networks such as those used with the 
backpropagation learning algorithm. The software is aimed both at 
the expert programmer/neural network researcher who may wish to tailor
significant portions of the system to his/her precise needs, as well
as at casual users who will wish to use the system with an absolute
minimum of effort.
<P>
<P>
<P>
<A NAME="DDLab"></A> <DT><B>DDLab</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.santafe.edu/~wuensch/ddlab.html">www.santafe.edu/~wuensch/ddlab.html</A></LI>
<LI>FTP site: 
<A HREF="ftp://ftp.santafe.edu/pub/wuensch/">ftp.santafe.edu/pub/wuensch/</A></LI>
</UL>
<P>DDLab is an interactive graphics program for research into the
dynamics of finite binary networks, relevant to the study of
complexity, emergent phenomena, neural networks, and aspects of
theoretical biology such as gene regulatory networks. A network
can be set up with any architecture between regular CA (1d or
2d) and "random Boolean networks" (networks with arbitrary
connections and heterogeneous rules). The network may also have
heterogeneous neighborhood sizes.
<P>
<P>
<A NAME="GENESIS"></A> <DT><B>GENESIS</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.bbb.caltech.edu/GENESIS/">www.bbb.caltech.edu/GENESIS/</A></LI>
<LI>FTP site: 
<A HREF="ftp://genesis.bbb.caltech.edu/pub/genesis/">genesis.bbb.caltech.edu/pub/genesis/</A></LI>
</UL>
   
<P>GENESIS (short for GEneral NEural SImulation System) is a
general purpose simulation platform which was developed to
support the simulation of neural systems ranging from complex
models of single neurons to simulations of large networks made
up of more abstract neuronal components. GENESIS has provided
the basis for laboratory courses in neural simulation at both
Caltech and the Marine Biological Laboratory in Woods Hole, MA,
as well as several other institutions. Most current GENESIS
applications involve realistic simulations of biological neural
systems. Although the software can also model more abstract
networks, other simulators are more suitable for backpropagation
and similar connectionist modeling.
<P>
<P>
<P>
<A NAME="JavaBayes"></A> <DT><B>JavaBayes</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.cs.cmu.edu/People/javabayes/index.html/">www.cs.cmu.edu/People/javabayes/index.html/</A></LI>
</UL>
<P>
<P>The JavaBayes system is a set of tools, containing a
graphical editor, a core inference engine and a parser.
JavaBayes can produce:
<UL>
<LI> the marginal distribution for any variable in a network.</LI>
<LI> the expectations for univariate functions (for example, 
expected value for variables).</LI>
<LI> configurations with maximum a posteriori probability.</LI>
<LI> configurations with maximum a posteriori expectation for 
univariate functions.</LI>
</UL>
<P>
<P>
<P>
<A NAME="Jbpe"></A> <DT><B>Jbpe</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://cs.felk.cvut.cz/~koutnij/studium/jbpe.html">cs.felk.cvut.cz/~koutnij/studium/jbpe.html</A></LI>
</UL>
<P>Jbpe is a back-propagation neural network editor/simulator.
<P>Features 
<UL>
<LI>Standart back-propagation networks creation. </LI>
<LI>Saving network as a text file, which can be edited and loaded 
back. </LI>
<LI>Saving/loading binary file </LI>
<LI>Learning from a text file (with structure specified below), 
number of learning periods / desired network energy can be
specified as a criterion. </LI>
<LI>Network recall </LI>
</UL>
<P>
<P>
<A NAME="neuralnets"></A> <DT><B>neuralnets</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://members.home.net/neuralnets/neuralnets/">members.home.net/neuralnets/neuralnets/</A></LI>
</UL>
<P>neuralnets is a text-based program that allows someone to build,
configure, train and run a Neural Network application. The code is
written in Java and is easily extended or included within other code.
The application comes ready to go with a Back Prop algorithm included.
Well known applications...stock market, weather prediction, scheduling,
image recognition, expert systems, research...basically anywhere you
may need to make a complicated decision.
<P>
<P>
<P>
<A NAME="NN Generator"></A> <DT><B>Neural Network Generator</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.idsia.ch/~rafal/research.html">www.idsia.ch/~rafal/research.html</A></LI>
<LI>FTP site: 
<A HREF="ftp://ftp.idsia.ch/pub/rafal/">ftp.idsia.ch/pub/rafal</A></LI>
</UL>
<P>The Neural Network Generator is a genetic algorithm for the
topological optimization of feedforward neural networks. It
implements the Semantic Changing Genetic Algorithm and the
Unit-Cluster Model. The Semantic Changing Genetic Algorithm is
an extended genetic algorithm that allows fast dynamic
adaptation of the genetic coding through population
analysis. The Unit-Cluster Model is an approach to the
construction of modular feedforward networks with a ''backbone''
structure. 
<P>
<P>NOTE: To compile this on Linux requires one change in the Makefiles. 
You will need to change '-ltermlib' to '-ltermcap'.
<P>
<P>
<P>
<A NAME="Neureka ANS"></A> <DT><B>Neureka ANS (nn/xnn)</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.bgif.no/neureka/">www.bgif.no/neureka/</A></LI>
<LI>FTP site: 
<A HREF="ftp://ftp.ii.uib.no/pub/neureka/">ftp.ii.uib.no/pub/neureka/</A></LI>
</UL>
 
<P>
<P>nn is a high-level neural network specification language. The
current version is best suited for feed-forward nets, but
recurrent models can and have been implemented, e.g. Hopfield
nets, Jordan/Elman nets, etc.  In nn, it is easy to change
network dynamics. The nn compiler can generate C code or
executable programs (so there must be a C compiler available),
with a powerful command line interface (but everything may also
be controlled via the graphical interface, xnn). It is possible
for the user to write C routines that can be called from inside
the nn specification, and to use the nn specification as a
function that is called from a C program. Please note that no
programming is necessary in order to use the network models that
come with the system (`netpack').  
<P>
<P>xnn is a graphical front end to networks generated by the nn
compiler, and to the compiler itself. The xnn graphical
interface is intuitive and easy to use for beginners, yet
powerful, with many possibilities for visualizing network data.
<P>
<P>NOTE: You have to run the install program that comes with this
to get the license key installed. It gets put (by default) in
/usr/lib. If you (like myself) want to install the package
somewhere other than in the /usr directory structure (the
install program gives you this option) you will have to set up
some environmental variables (NNLIBDIR &amp; NNINCLUDEDIR are
required). You can read about these (and a few other optional
variables) in appendix A of the documentation (pg 113).
<P>
<P>
<P>
<A NAME="NEURON"></A> <DT><B>NEURON</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.neuron.yale.edu/neuron.html">www.neuron.yale.edu/neuron.html</A></LI>
<LI>FTP site: 
<A HREF="ftp://ftp.neuron.yale.edu/neuron/unix/">ftp.neuron.yale.edu/neuron/unix/</A></LI>
</UL>
<P>NEURON is an extensible nerve modeling and simulation
program. It allows you to create complex nerve models by
connecting multiple one-dimensional sections together to form
arbitrary cell morphologies, and allows you to insert multiple
membrane properties into these sections (including channels,
synapses, ionic concentrations, and counters). The interface was
designed to present the neural modeler with a intuitive
environment and hide the details of the numerical methods used
in the simulation.
<P>
<P>
<P>
<A NAME="PDP++"></A> <DT><B>PDP++</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.cnbc.cmu.edu/PDP++/">www.cnbc.cmu.edu/PDP++/</A></LI>
<LI>FTP site (US): 
<A HREF="ftp://cnbc.cmu.edu/pub/pdp++/">cnbc.cmu.edu/pub/pdp++/</A></LI>
<LI>FTP site (Europe): 
<A HREF="ftp://unix.hensa.ac.uk/mirrors/pdp++/">unix.hensa.ac.uk/mirrors/pdp++/</A></LI>
</UL>
<P>
<P>As the field of Connectionist modeling has grown, so has the need
for a comprehensive simulation environment for the development and
testing of Connectionist models. Our goal in developing PDP++ has been
to integrate several powerful software development and user interface
tools into a general purpose simulation environment that is both user
friendly and user extensible. The simulator is built in the C++
programming language, and incorporates a state of the art script
interpreter with the full expressive power of C++. The graphical user
interface is built with the Interviews toolkit, and allows full access
to the data structures and processing modules out of which the
simulator is built. We have constructed several useful graphical
modules for easy interaction with the structure and the contents of
neural networks, and we've made it possible to change and adapt many
things. At the programming level, we have set things up in such a way
as to make user extensions as painless as possible. The programmer
creates new C++ objects, which might be new kinds of units or new
kinds of processes; once compiled and linked into the simulator, these
new objects can then be accessed and used like any other.
<P>
<P>
<P>
<A NAME="RNS"></A> <DT><B>RNS</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/rns/">www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/rns/</A></LI>
</UL>
<P>RNS (Recurrent Network Simulator) is a simulator for recurrent
neural networks. Regular neural networks are also supported. The
program uses a derivative of the back-propagation algorithm, but
also includes other (not that well tested) algorithms.
<P>Features include
<UL>
<LI>freely choosable connections, no restrictions besides memory
or CPU constraints</LI>
<LI>delayed links for recurrent networks</LI>
<LI>fixed values or thresholds can be specified for weights</LI>
<LI>(recurrent) back-propagation, Hebb, differential Hebb, simulated
annealing and more  </LI>
<LI>patterns can be specified with bits, floats, characters, numbers, 
and random bit patterns with Hamming distances can be chosen for you</LI>
<LI>user definable error functions</LI>
<LI>output results can be used without modification as input</LI>
</UL>
<P>
<P>
<P>
<A NAME="Python ANN"></A> <DT><B>Simple Neural Net (in Python)</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://starship.python.net/crew/amk/unmaintained/">starship.python.net/crew/amk/unmaintained/</A></LI>
</UL>
<P>
<P>Simple neural network code, which implements a class for 3-level
networks (input, hidden, and output layers). The only learning
rule implemented is simple backpropagation. No documentation (or
even comments) at all, because this is simply code that I use to
experiment with. Includes modules containing sample datasets
from Carl G. Looney's NN book. Requires the Numeric
extensions. 
<P>
<P>
<P>
<A NAME="SCNN"></A> <DT><B>SCNN</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://apx00.physik.uni-frankfurt.de/e_ag_rt/SCNN/">apx00.physik.uni-frankfurt.de/e_ag_rt/SCNN/</A></LI>
</UL>
<P>
<P>SCNN is an universal simulating system for Cellular Neural
Networks (CNN).  CNN are analog processing neural networks
with regular and local interconnections, governed by a set of
nonlinear ordinary differential equations. Due to their local
connectivity, CNN are realized as VLSI chips, which operates
at very high speed.
<P>
<P>
<P>
<A NAME="Python Smantic Nets"></A> <DT><B>Semantic Networks in Python</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://strout.net/info/coding/python/ai/index.html">strout.net/info/coding/python/ai/index.html</A></LI>
</UL>
<P>
<P>The semnet.py module defines several simple classes for
building and using semantic networks.  A semantic network is a
way of representing knowledge, and it enables the program to
do simple reasoning with very little effort on the part of the
programmer.
<P>
<P>The following classes are defined:
<UL>
<LI><B>Entity</B>: This class represents a noun; it is
something which can be related to other things, and about
which you can store facts.</LI>
<LI><B>Relation</B>: A Relation is a type of relationship
which may exist between two entities.  One special relation,
"IS_A", is predefined because it has special meaning (a sort
of logical inheritance).</LI>
<LI><B>Fact</B>: A Fact is an assertion that a relationship 
exists between two entities.</LI>
</UL>
<P>
<P>With these three object types, you can very quickly define knowledge 
about a set of objects, and query them for logical conclusions.
<P>
<P>
<P>
<A NAME="SNNS"></A> <DT><B>SNNS</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/">www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/</A></LI>
<LI>FTP site: 
<A HREF="ftp://ftp.informatik.uni-stuttgart.de/pub/SNNS/">ftp.informatik.uni-stuttgart.de/pub/SNNS/</A></LI>
</UL>
<P>Stuttgart Neural Net Simulator (version 4.1).  An awesome neural
net simulator. Better than any commercial simulator I've seen. The
simulator kernel is written in C (it's fast!). It supports over 20
different network architectures, has 2D and 3D X-based graphical
representations, the 2D GUI has an integrated network editor, and can
generate a separate NN program in C. SNNS is very powerful, though
a bit difficult to learn at first. To help with this it comes with
example networks and tutorials for many of the architectures.
ENZO, a supplementary system allows you to evolve your networks with
genetic algorithms.
<P>
<P>There is a 
<A HREF="http://cgi.debian.org/cgi-bin/search_packages.pl?keywords=snns&amp;searchon=names&amp;version=stable&amp;release=all">debian package of SNNS</A> available. So just get it 
(and use 
<A HREF="http://kitenet.net/programs/alien/">alien</A>
to convert it to RPM if you need to).
<P>
<P>
<P>
<A NAME="SPRLIB-ANNLIB"></A> <DT><B>SPRLIB/ANNLIB</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.ph.tn.tudelft.nl/~sprlib/">www.ph.tn.tudelft.nl/~sprlib/</A></LI>
</UL>
<P>
<P>SPRLIB (Statistical Pattern Recognition Library) was developed
to support the easy construction and simulation of pattern
classifiers. It consist of a library of functions (written in C)
that can be called from your own program. Most of the well-known
classifiers are present (k-nn, Fisher, Parzen, ....), as well as
error estimation and dataset generation routines.
<P>
<P>ANNLIB (Artificial Neural Networks Library) is a neural network
simulation library based on the data architecture laid down by
SPRLIB. The library contains numerous functions for creating,
training and testing feed-forward networks.  Training algorithms
include back-propagation, pseudo-Newton, Levenberg-Marquardt,
conjugate gradient descent, BFGS.... Furthermore, it is possible
- due to the datastructures' general applicability - to build
Kohonen maps and other more exotic network architectures using
the same data types.
<P>
<P>
<P>
<P>
<A NAME="TOOLDIAG"></A> <DT><B>TOOLDIAG</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.inf.ufes.br/~thomas/www/home/tooldiag.html">www.inf.ufes.br/~thomas/www/home/tooldiag.html</A></LI>
<LI>FTP site: 
<A HREF="ftp://ftp.inf.ufes.br/pub/tooldiag/">ftp.inf.ufes.br/pub/tooldiag/</A></LI>
</UL>
<P>TOOLDIAG is a collection of methods for statistical pattern
recognition. The main area of application is classification. The
application area is limited to multidimensional continuous
features, without any missing values. No symbolic features
(attributes) are allowed. The program in implemented in the 'C'
programming language and was tested in several computing
environments. 
<P>
<P>
<A NAME="XNBC"></A> <DT><B>XNBC</B><DD><P>
<UL>
<LI>Web site: 
<A HREF="http://www.b3e.jussieu.fr/xnbc/">www.b3e.jussieu.fr/xnbc/</A></LI>
</UL>
<P>XNBC v8 is a simulation tool for the neuroscientists interested in
simulating biological neural networks using a user friendly tool. 
<P>XNBC is a software package for simulating biological neural networks. 
<P>Four neuron models are available, three phenomenologic models (xnbc, 
leaky integrator and conditional burster) and an ion-conductance based 
model. Inputs to the simulated neurons can be provided by experimental 
data stored in files, allowing the creation of `hybrid'' networks. 
<P>
<P>
<P>
</DL>
<P>
<P>
<HR>
<A HREF="AI-Alife-HOWTO-4.html"><IMG SRC="../img/next.gif" ALT="Next"></A>
<A HREF="AI-Alife-HOWTO-2.html"><IMG SRC="../img/prev.gif" ALT="Previous"></A>
<A HREF="AI-Alife-HOWTO.html#toc3"><IMG SRC="../img/toc.gif" ALT="Contents"></A>
<SCRIPT>EndPage();</SCRIPT>  </BODY>
</HTML>
