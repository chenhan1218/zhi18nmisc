<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<HTML>
<HEAD>
 <META NAME="GENERATOR" CONTENT="SGML-Tools 1.0.9">
 <TITLE>CPU Design HOW-TO: Super Computer Architecture</TITLE>
 <LINK HREF="CPU-Design-HOWTO-7.html" REL=next>
 <LINK HREF="CPU-Design-HOWTO-5.html" REL=previous>
 <LINK HREF="CPU-Design-HOWTO.html#toc6" REL=contents>
<SCRIPT src="../menu.js"> function BeginPage() {} function EndPage() {} </SCRIPT> </HEAD> <BODY bgcolor=#EEEEFF MARGINHEIGHT=0 MARGINWIDTH=0> <SCRIPT>BeginPage(1, 8, 1);</SCRIPT>
<A HREF="CPU-Design-HOWTO-7.html"><IMG SRC="../img/next.gif" ALT="Next"></A>
<A HREF="CPU-Design-HOWTO-5.html"><IMG SRC="../img/prev.gif" ALT="Previous"></A>
<A HREF="CPU-Design-HOWTO.html#toc6"><IMG SRC="../img/toc.gif" ALT="Contents"></A>
<HR>
<H2><A NAME="s6">6. Super Computer Architecture</A></H2>

<P>For building Super computers, the trend that seems to emerge is 
that most new systems look as minor 
variations on the same theme: clusters of RISC-based Symmetric 
Multi-Processing (SMP) nodes which in turn are connected by a fast
network. Consider this as a natural architectural evolution. 
The availability of relatively low-cost (RISC) processors and 
network products to connect these processors together with 
standardised communication software has stimulated the building 
of home-brew clusters computers as an alternative to complete 
systems offered by vendors.
<P>Visit the following sites for Super Computers -
<UL>
<LI> Top 500 super computers 
<A HREF="http://www.top500.org/ORSC/2000">http://www.top500.org/ORSC/2000</A></LI>
<LI> National Computing Facilities Foundation 
<A HREF="http://www.nwo.nl/ncf/indexeng.htm">http://www.nwo.nl/ncf/indexeng.htm</A></LI>
<LI> Linux Super Computer Beowulf cluster 
<A HREF="http://www.linuxdoc.org/HOWTO/Beowulf-HOWTO.html">http://www.linuxdoc.org/HOWTO/Beowulf-HOWTO.html</A></LI>
<LI> Extreme machines - beowulf cluster 
<A HREF="http://www.xtreme-machines.com">http://www.xtreme-machines.com</A></LI>
<LI> System architecture description of
the Hitachi SR2201 
<A HREF="http://www.hitachi.co.jp/Prod/comp/hpc/eng/sr1.html">http://www.hitachi.co.jp/Prod/comp/hpc/eng/sr1.html</A></LI>
<LI> Personal Parallel Supercomputers 
<A HREF="http://www.checs.net/checs_98/papers/super">http://www.checs.net/checs_98/papers/super</A></LI>
</UL>
<H2><A NAME="ss6.1">6.1 Main Architectural Classes</A>
</H2>

<P>Before going on to the descriptions of the machines themselves, it is 
important to consider some mechanisms that are or have been used to 
increase the performance. The hardware structure or architecture 
determines to a large extent what the possibilities and impossibilities 
are in speeding up a computer system beyond the performance of a single 
CPU. Another important factor that is considered in combination with 
the hardware is the capability of compilers to generate efficient code
to be executed on the given hardware platform. In many cases it is hard
to distinguish between hardware and software influences and one has to be 
careful in the interpretation of results when ascribing certain effects 
to hardware or software peculiarities or both. In this chapter we will 
give most emphasis to the hardware architecture. For a description of
machines that can be considered to be classified as "high-performance".
<P>Since many years the taxonomy of Flynn has proven to be useful for
the classification of high-performance computers. This classification 
is based on the way of manipulating of instruction and data streams and 
comprises four main architectural classes. We will first briefly sketch 
these classes and afterwards fill in some details when each of the 
classes is described. 
<H2><A NAME="ss6.2">6.2 SISD machines </A>
</H2>

<P>These are the conventional systems that contain one CPU 
and hence can accommodate one instruction stream that is executed serially.
Nowadays many large mainframes may have more than one CPU but each of 
these execute instruction streams that are unrelated. Therefore, such 
systems still should be regarded as (a couple of) SISD machines acting 
on different data spaces. Examples of SISD machines are for instance 
most workstations like those of DEC, Hewlett-Packard, and Sun 
Microsystems. The definition of SISD machines is given here for 
completeness' sake. We will not discuss this type of machines 
in this report. 
<H2><A NAME="ss6.3">6.3 SIMD machines</A>
</H2>

<P>Such systems often have a large number of processing 
units, ranging from 1,024 to 16,384 that all may execute the same 
instruction on different data in lock-step. So, a single instruction 
manipulates many data items in parallel. Examples of SIMD machines 
in this class are the CPP DAP Gamma II and the Alenia Quadrics. 
<P>Another subclass of the SIMD systems are the vectorprocessors. 
Vectorprocessors act on arrays of similar data rather than on single 
data items using specially structured CPUs. When data can be manipulated
by these vector units, results can be delivered with a rate of one,
two and --- in special cases --- of three per clock cycle (a clock 
cycle being defined as the basic internal unit of time for the system).
So, vector processors execute on their data in an almost parallel way
but only when executing in vector mode. In this case they are several
times faster than when executing in conventional scalar mode. For 
practical purposes vectorprocessors are therefore mostly regarded 
as SIMD machines. Examples of such systems is for instance 
the Hitachi S3600. 
<H2><A NAME="ss6.4">6.4 MISD machines</A>
</H2>

<P>Theoretically in these type of machines multiple 
instructions should act on a single stream of data. As yet no
practical machine in this class has been constructed nor are 
such systems easily to conceive. We will disregard them in the 
following discussions. 
<H2><A NAME="ss6.5">6.5 MIMD machines</A>
</H2>

<P>These machines execute several instruction 
streams in parallel on different data. The difference with the
multi-processor SISD machines mentioned above lies in the fact that 
the instructions and data are related because they represent different 
parts of the same task to be executed. So, MIMD systems may run 
many sub-tasks in parallel in order to shorten the time-to-solution
for the main task to be executed. There is a large variety of 
MIMD systems and especially in this class the Flynn taxonomy proves 
to be not fully adequate for the classification of systems. Systems 
that behave very differently like a four-processor NEC SX-5 and a
thousand processor SGI/Cray T3E fall both in this class. In the 
following we will make another important distinction between classes 
of systems and treat them accordingly. 
<H3>Shared memory systems</H3>

<P>Shared memory systems have multiple CPUs all
of which share the same address space. This means that the knowledge 
of where data is stored is of no concern to the user as there is only
one memory accessed by all CPUs on an equal basis. Shared memory 
systems can be both SIMD or MIMD. Single-CPU vector processors can be 
regarded as an example of the former, while the multi-CPU models of 
these machines are examples of the latter. We will sometimes use the 
abbreviations SM-SIMD and SM-MIMD for the two subclasses. 
<H3>Distributed memory systems</H3>

<P>In this case each CPU has its own
associated memory. The CPUs are connected by some network and may
exchange data between their respective memories when required. In 
contrast to shared memory machines the user must be aware of the
location of the data in the local memories and will have to move
or distribute these data explicitly when needed. Again, distributed 
memory systems may be either SIMD or MIMD. The first class of
SIMD systems mentioned which operate in lock step, all have distributed
memories associated to the processors. As we will see, 
distributed-memory MIMD systems exhibit a large variety in the 
topology of their connecting network. The details of this topology 
are largely hidden from the user which is quite helpful with 
respect to portability of applications. For the distributed-memory 
systems we will sometimes use DM-SIMD and DM-MIMD to indicate
the two subclasses. 
Although the difference between shared- and distributed memory 
machines seems clear cut, this is not always entirely the case
from user's point of view. For instance, the late Kendall 
Square Research systems employed the idea of "virtual shared memory" 
on a hardware level. Virtual shared memory can also be simulated 
at the programming level: A specification of High Performance 
Fortran (HPF) was published in 1993 which by means of 
compiler directives distributes the data over the 
available processors. Therefore, the system on which HPF is
implemented in this case will look like a shared memory machine 
to the user. Other vendors of Massively Parallel Processing 
systems (sometimes called MPP systems), like HP 
and SGI/Cray,
also are able to support proprietary virtual shared-memory programming models due to 
the fact that these physically distributed memory systems are able to address 
the whole collective address space. So, for 
the user such systems have one global address space spanning all of 
the memory in 
the system. We will say a little more about 
the structure of such systems in 
the ccNUMA section. In addition, packages like TreadMarks
provide a virtual shared memory environment for networks of workstations. 
<H2><A NAME="ss6.6">6.6 Distributed Processing Systems</A>
</H2>

<P>Another trend that has came up in 
the last few years is distributed processing. This takes 
the DM-MIMD concept one step further: instead 
of many integrated processors in one or several boxes, 
workstations, mainframes, etc., are connected by (Gigabit) Ethernet, FDDI, or otherwise 
and set to work concurrently on tasks in
the same program. Conceptually, this is not different from DM-MIMD computing, but
the communication between processors is often orders 
of magnitude slower. Many packages to realise distributed 
computing are available. Examples of
these are PVM (st
anding for Parallel Virtual Machine), 
and MPI (Message Passing Interface). This style 
of programming, called
the "message passing" model has becomes so much accepted that PVM 
and MPI have been adopted by virtually all major vendors 
of distributed-memory MIMD systems 
and even on shared-memory MIMD systems for compatibility reasons. In addition
there is a tendency to cluster shared-memory systems, 
for instance by HiPPI channels, to obtain systems 
with a very high computational power. E.g.,
the NEC SX-5, 
and
the SGI/Cray SV1 have this structure. So, within
the clustered nodes a shared-memory programming style can be 
used while between clusters message-passing should be used. 
<H2><A NAME="ss6.7">6.7 ccNUMA machines</A>
</H2>

<P>As already mentioned in the introduction, a trend can be 
observed to build systems that have a rather small (up to 16)
number of RISC processors that are tightly integrated in 
a cluster, a Symmetric Multi-Processing (SMP) node. The
processors in such a node are virtually always connected 
by a 1-stage crossbar while these clusters are connected by a
less costly network. 
<P>This is similar to the policy mentioned for large 
vectorprocessor ensembles mentioned above but with the important
difference that all of the processors can access all of the 
address space. Therefore, such systems can be considered as
SM-MIMD machines. On the other hand, because the memory is 
physically distributed, it cannot be guaranteed that a
data access operation always will be satisfied within the same 
time. Therefore such machines are called ccNUMA
systems where ccNUMA stands for Cache Coherent Non-Uniform Memory 
Access. The term "Cache Coherent" refers
to the fact that for all CPUs any variable that is to be used
must have a consistent value. Therefore, is must be assured
that the caches that provide these variables are also consistent 
in this respect. There are various ways to ensure that the
caches of the CPUs are coherent. One is the snoopy bus 
protocol in which the caches listen in on transport of variables to
any of the CPUs and update their own copies of these 
variables if they have them. Another way is the directory memory,
a special part of memory which enables to keep track of the all 
copies of variables and of their validness. 
<P>For all practical purposes we can classify these systems as 
being SM-MIMD machines also because special assisting
hardware/software (such as a directory memory) has been
incorporated to establish a single system image although
the memory is physically distributed.
<HR>
<A HREF="CPU-Design-HOWTO-7.html"><IMG SRC="../img/next.gif" ALT="Next"></A>
<A HREF="CPU-Design-HOWTO-5.html"><IMG SRC="../img/prev.gif" ALT="Previous"></A>
<A HREF="CPU-Design-HOWTO.html#toc6"><IMG SRC="../img/toc.gif" ALT="Contents"></A>
<SCRIPT>EndPage();</SCRIPT>  </BODY>
</HTML>
